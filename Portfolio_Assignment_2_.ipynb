{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PpYrMm83vZE"
      },
      "source": [
        "# Binary Classification with IMDb Dataset\n",
        "\n",
        "## Introduction\n",
        "In this notebook, I will be working with the **IMDb movie review dataset** from **Hugging Face**. This dataset consists of **two classes**: **positive** and **negative** reviews, making it a **binary classification task**.\n",
        "\n",
        "## Objective\n",
        "The goal is to **fine-tune a transformer-based pre-trained model** to classify movie reviews as either **positive or negative**. This approach leverages **transfer learning** to improve performance with minimal training time.\n",
        "\n",
        "## Workflow:\n",
        "1. **Load the IMDb dataset** from Hugging Face.\n",
        "2. **Preprocess the text** (tokenization using a transformer tokenizer).\n",
        "3. **Fine-tune a transformer model** for sentiment classification.\n",
        "4. **Evaluate performance** using accuracy, F1-score, and other metrics.\n",
        "5. **Deployment using Gradio**, this allows users to input text and see classification predictions.\n",
        "\n",
        "\n",
        "### Let's get started! ðŸš€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah9kPCcj2nxu"
      },
      "outputs": [],
      "source": [
        "!pip install datasets --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwnWKR9S0kck"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg_WeWFO2s2K"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlFNpuIpUwt1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute word counts for each example in the training set\n",
        "def compute_word_count(example):\n",
        "    example[\"word_count\"] = len(example[\"text\"].split())\n",
        "    return example\n",
        "\n",
        "ds_train_word_counts = ds[\"train\"].map(compute_word_count)\n",
        "word_counts = ds_train_word_counts[\"word_count\"]\n",
        "\n",
        "# Plot the distribution\n",
        "plt.hist(word_counts, bins=50)\n",
        "plt.xlabel(\"Word Count\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Word Count Distribution in Train Set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTH41AV4UO33"
      },
      "outputs": [],
      "source": [
        "def filter_under_300_words(example):\n",
        "    return len(example[\"text\"].split()) < 300\n",
        "\n",
        "# Apply filtering to the full dataset\n",
        "filtered_dataset = ds.filter(filter_under_300_words)\n",
        "\n",
        "# Split into train and validation\n",
        "filtered_train, filtered_val = filtered_dataset[\"train\"].train_test_split(test_size=0.2, seed=42).values()\n",
        "\n",
        "# Shuffle & select a smaller subset for training & validation\n",
        "small_train_dataset = filtered_train.shuffle(seed=42).select(range(3000))\n",
        "small_val_dataset = filtered_val.shuffle(seed=42).select(range(3000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7QwkegUU4YA"
      },
      "source": [
        "We will be limiting the word count since the model that will be used in the task has a token limit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LuTsCiHN2OR"
      },
      "outputs": [],
      "source": [
        "small_train_dataset[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faCn-zUdIX-0"
      },
      "outputs": [],
      "source": [
        "small_val_dataset[5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEvW-5RvIyP8"
      },
      "outputs": [],
      "source": [
        "train_labels = small_train_dataset[\"label\"]\n",
        "val_labels = small_val_dataset[\"label\"]\n",
        "\n",
        "# Using Counter to count occurrences of each class\n",
        "from collections import Counter\n",
        "\n",
        "train_class_distribution = Counter(train_labels)\n",
        "val_class_distribution = Counter(val_labels)  # Renamed from eval_class_distribution\n",
        "\n",
        "print(\"Train Class Distribution:\", train_class_distribution)\n",
        "print(\"Validation Class Distribution:\", val_class_distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvwF3zr0I5_c"
      },
      "source": [
        "This indicates that the classes are fairly balanced in both the training and evaluation datasets, we don't need to address class imbalance since the dataset seems evenly distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhPZUfKO4vZT"
      },
      "source": [
        "## Preprocess the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WppEb_mM4wgL"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f9Hy4Nw56_w"
      },
      "source": [
        "We will be using BERT which is a transformers model pretrained on a large corpus of English data in a self-supervised fashion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXQ9tJVL42mL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Define model name\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# BERT is great for text understanding, and \"bert-base-uncased\" is lightweight yet powerful.\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Test the tokenizer with a sample sentence\n",
        "sample_text = \"This movie was absolutely fantastic!\"\n",
        "tokens = tokenizer(sample_text)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lx0RDPMJFr9"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
        "\n",
        "# Apply tokenization efficiently\n",
        "tokenized_small_train_dataset = small_train_dataset.map(\n",
        "    tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "tokenized_small_val_dataset = small_val_dataset.map(\n",
        "    tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P5QfI8M8cvY"
      },
      "outputs": [],
      "source": [
        "# Convert dataset to PyTorch format\n",
        "columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"]\n",
        "\n",
        "tokenized_small_train_dataset.set_format(type=\"torch\", columns=columns)\n",
        "tokenized_small_val_dataset.set_format(type=\"torch\", columns=columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGc0t0i2J6U4"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efR6zq8JabE5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz_KxcOVabtD"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_small_train_dataset,\n",
        "    shuffle=True,\n",
        "    batch_size=8,\n",
        "    drop_last=True,  # Prevents last batch size mismatches\n",
        "    pin_memory=True if torch.cuda.is_available() else False  # Optimizes GPU usage\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    tokenized_small_val_dataset,\n",
        "    batch_size=8,\n",
        "    drop_last=False  # Keep all validation samples\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3yjuEC3aenl"
      },
      "outputs": [],
      "source": [
        "# load a pre-trained model\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "# This automatically replaces BERTâ€™s original output layer with a new classification layer for the task.\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mctq3yI3gwNP"
      },
      "source": [
        " This output confirms that your BERT model is set up for binary classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kr-hQETl6Dm"
      },
      "source": [
        "In order to have full control over the training process and gain deeper understanding of how training works, we will use `manual training`. This could have also been done with Trainer API which automates what we're doing manually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_XoresPagwM"
      },
      "outputs": [],
      "source": [
        "# Weight decay helps regularize the model and reduce overfitting.\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "num_epochs = 5\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbFKumNXgwya"
      },
      "source": [
        "This setup ensures that the model training is regularized through weight decay, runs for a defined number of steps, and uses a learning rate that decays gradually to help stabilize training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQxsgzAGemQH"
      },
      "source": [
        "## Full Training Loop for Fine-Tuning BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFwUxJ26aiYU"
      },
      "outputs": [],
      "source": [
        "# Early stopping parameters to prevent overfitting\n",
        "best_val_loss = float(\"inf\")\n",
        "patience = 2  # Number of epochs with no improvement to wait before stopping\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    # Training phase\n",
        "    for batch in progress_bar:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"label\"]\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1} average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"]\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            logits = outputs.logits\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(batch[\"label\"].cpu().numpy())\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch+1} average validation loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    f1 = f1_score(all_labels, all_predictions, average=\"binary\")\n",
        "    precision = precision_score(all_labels, all_predictions, average=\"binary\")\n",
        "    recall = recall_score(all_labels, all_predictions, average=\"binary\")\n",
        "    print(\"Validation Metrics:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  F1 Score:  {f1:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall:    {recall:.4f}\")\n",
        "\n",
        "    # Early stopping check: if validation loss doesn't improve, increment counter.\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0  # Reset counter if improvement\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model when it improves\n",
        "        print(\"New best model saved!\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\" Early stopping triggered\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HcqwUG8C5tm"
      },
      "source": [
        "## Model test on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDDoQWvjCyVZ"
      },
      "outputs": [],
      "source": [
        "# Filter original test split\n",
        "filtered_test = ds[\"test\"].map(compute_word_count).filter(filter_under_300_words)\n",
        "\n",
        "# Shuffle & subset\n",
        "small_test_dataset = filtered_test.shuffle(seed=42).select(range(3000))\n",
        "\n",
        "# Tokenize & remove 'text'\n",
        "tokenized_small_test_dataset = small_test_dataset.map(\n",
        "    tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"]\n",
        ")\n",
        "\n",
        "# Set format\n",
        "tokenized_small_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\", \"label\"])\n",
        "\n",
        "# Create test dataloader\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_small_test_dataset,\n",
        "    batch_size=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ95RLWv_r1J"
      },
      "outputs": [],
      "source": [
        "# Load the best saved model before testing\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists for test predictions and labels\n",
        "test_predictions = []\n",
        "test_labels = []\n",
        "\n",
        "# Run model on the test dataset\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(\n",
        "            input_ids=batch[\"input_ids\"],\n",
        "            attention_mask=batch[\"attention_mask\"],\n",
        "            labels=batch[\"label\"]\n",
        "        )\n",
        "        preds = torch.argmax(outputs.logits, dim=-1)\n",
        "        test_predictions.extend(preds.cpu().numpy())\n",
        "        test_labels.extend(batch[\"label\"].cpu().numpy())\n",
        "\n",
        "# Compute final metrics on test set\n",
        "test_accuracy = accuracy_score(test_labels, test_predictions)\n",
        "test_f1 = f1_score(test_labels, test_predictions, average=\"binary\")\n",
        "\n",
        "print(f\" Final Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\" Final Test F1 Score: {test_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He6830zcAsLT"
      },
      "source": [
        "For sentiment analysis (binary classification), anything above ~85% is strong.\n",
        "\n",
        "F1 Score (0.8803) is high, meaning the model balances precision & recall well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZimyN5Q4ezJ7"
      },
      "source": [
        "### Quick test on unseen reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHKj6ChWcQoo"
      },
      "outputs": [],
      "source": [
        "# Load the best saved model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Sample unseen reviews\n",
        "new_reviews = [\n",
        "    \"I absolutely loved this movie!\",\n",
        "    \"The film was boring and too long\",\n",
        "    \"An average experience, nothing spectacular\"\n",
        "]\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(new_reviews, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "# Map labels\n",
        "labels = [\"Negative\", \"Positive\"]\n",
        "results = [labels[pred] for pred in predictions]\n",
        "\n",
        "# Print results\n",
        "for review, result in zip(new_reviews, results):\n",
        "    print(f\"Review: {review}\\nPrediction: {result}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0KboiliepoV"
      },
      "source": [
        "0 for negative, 1 for positive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbR9BaGFC6Ys"
      },
      "source": [
        "## Uploading on HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7B7kB_SijF1"
      },
      "outputs": [],
      "source": [
        "# !pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81KslHdUime3"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypfn28L4jKkQ"
      },
      "outputs": [],
      "source": [
        "# model.push_to_hub(\"Lpiziks2/imdb-bert-finetuned\")\n",
        "# tokenizer.push_to_hub(\"Lpiziks2/imdb-bert-finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXt00cMWe2oz"
      },
      "source": [
        "## Deployment using Gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUlS9DyaiJfh"
      },
      "outputs": [],
      "source": [
        "!pip install gradio --q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFESQa24iAB0"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load the best saved model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "def classify_text(text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        probabilities = F.softmax(outputs.logits, dim=-1)  # Convert logits to probabilities\n",
        "        prediction = torch.argmax(probabilities, dim=-1).item()\n",
        "        confidence = probabilities[0][prediction].item()  # Get confidence score\n",
        "\n",
        "    # Map prediction to a label\n",
        "    label = \"Positive\" if prediction == 1 else \"Negative\"\n",
        "    return f\"{label} ({confidence:.2f} confidence)\"\n",
        "\n",
        "# Create the Gradio interface with probability output\n",
        "interface = gr.Interface(\n",
        "    fn=classify_text,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter a movie review here...\"),\n",
        "    outputs=gr.Textbox(label=\"Prediction\"),\n",
        "    title=\"Movie Review Sentiment Classifier\",\n",
        "    description=\"Enter a movie review and the model will predict whether the sentiment is Positive or Negative, along with confidence.\"\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}